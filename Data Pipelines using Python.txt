Building data pipeline using python

1. Data sources		--> operational data bases that store transactions
2. Target areas 	--> a collection of databases called data lakes
3. Data lakes 		--> consist of 3 main regions:
				a. Landing area 	--> area of truth. This is where data is stored as it is received
				b. Clean area		--> area where clean area resides
				c. Business area	--> area where special transformations are applied to data to match the domain requirement
4. A data lake is usually in the form of a FILE MANAGEMENT SYSTEM -- Unix / Linux / DOS etc..
5. DATA CATALOG 	--> To navigate a data lake, sometimes organisations provide DATA CATALOGS. This stores information on where the data resides and the format in which the data is being stored.
6. SPARK DATAFRAME 	--> Spark data frame is a distributed collection of data. They are often used for big data
7. SINGER		--> Data ingestion specification
				a. Data ingestion script --> tap
				b. Data load script --> target
				c. They communicate via:
					-- schema (table meta data)
					-- stream (process meta data)
					-- records (data)
				d. Data exchange format --> JSON

JSON --
	javascript object notation
	syntax for storing and exchanging data
	
JSON package in python

1. You can use the methods within the json package by invoking the package name
	import json

2. To serialise a JSON you can do it 2 manners. 
	a. By using the .dump() method. To use this, we must use the following syntax.
		
		EXAMPLE: 
		with open("data_file.json","w") as write_file:
			json.dump( obj = "<name of JSON variable in the computer memory between double quotes>",
				   fp  = "<name of the file the value within the JSON variable will be serialised. In this case it is write_file>")
	b, By using the .dumps() method. This method makes the code easier to read and implicitly performs the above steps.
		EXAMPLE:
		json_string = json.dumps(obj = "name of JSON variable",
					 fp = "write_file")

Running an ingestion pipeline with Singer 		
1. To generate a singer record, we called the write_record() function
2. When we combine the 'write_schema' and 'write_record' functions, we have a tap module in python which outputs to the console.
3. If we have a target that can parse this tap output, we have a full ingestion pipeline.

Getting data in to data lake
1. With the boom of IoT, interacting with web API is becoming a common way to ingest data in to the data lake. Need to do further study in to this.
					

REST
REST is essentially a set of useful conventions for structuring a web API
	Web API 	--> it is an application interface that you use to interact with a web URL, to get relevant data in return 
	GET method	--> Get method is used to get some information about an object in the website. Get method is not expected to make any change to that information
	POST method	--> Post method in contrast allows us to make change to the data. 